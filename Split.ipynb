{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f72f68",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ce211d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total .npy encontrados: 696\n",
      "train: 487 archivos\n",
      "val: 139 archivos\n",
      "test: 70 archivos\n",
      "Listas creadas en /home/matiaslein/ProyectoFinalVA/img_splits (train.txt, val.txt, test.txt)\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "from pathlib import Path\n",
    "\n",
    "SRC_ROOT = Path.home() / \"TBBR\" / \"train\" / \"images\"\n",
    "SUBSETS = [\"Flug1_100\", \"Flug1_104\"]\n",
    "DEST_ROOT = Path.home() / \"ProyectoFinalVA\" / \"img_splits\"\n",
    "EXT = \".npy\"\n",
    "SPLIT = (0.7, 0.2, 0.1)\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "DEST_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Recolectar todas las rutas .npy\n",
    "paths = []\n",
    "for sub in SUBSETS:\n",
    "    folder = SRC_ROOT / sub\n",
    "    if folder.exists():\n",
    "        paths += list(folder.glob(f\"*{EXT}\"))\n",
    "\n",
    "print(f\"Total .npy encontrados: {len(paths)}\")\n",
    "paths = sorted(paths)\n",
    "random.shuffle(paths)\n",
    "\n",
    "# 2) Split\n",
    "n = len(paths)\n",
    "n_train = int(n * SPLIT[0])\n",
    "n_val   = int(n * SPLIT[1])\n",
    "splits = {\n",
    "    \"train\": paths[:n_train],\n",
    "    \"val\":   paths[n_train:n_train+n_val],\n",
    "    \"test\":  paths[n_train+n_val:]\n",
    "}\n",
    "\n",
    "# 3) Escribir listas\n",
    "for split, lst in splits.items():\n",
    "    out = DEST_ROOT / f\"{split}.txt\"\n",
    "    with open(out, \"w\") as f:\n",
    "        for p in lst:\n",
    "            f.write(str(p.resolve()) + \"\\n\")\n",
    "    print(f\"{split}: {len(lst)} archivos\")\n",
    "\n",
    "print(f\"Listas creadas en {DEST_ROOT} (train.txt, val.txt, test.txt)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9279f207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] images=439 anns=3405 -> /home/matiaslein/ProyectoFinalVA/annotations_split/train.json\n",
      "[train] YOLO .txt escritos en /home/matiaslein/ProyectoFinalVA/YOLO/HST/labels/train\n",
      "[val] images=155 anns=1242 -> /home/matiaslein/ProyectoFinalVA/annotations_split/val.json\n",
      "[val] YOLO .txt escritos en /home/matiaslein/ProyectoFinalVA/YOLO/HST/labels/val\n",
      "[test] images=90 anns=714 -> /home/matiaslein/ProyectoFinalVA/annotations_split/test.json\n",
      "[test] YOLO .txt escritos en /home/matiaslein/ProyectoFinalVA/YOLO/HST/labels/test\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= CONFIG =========\n",
    "BASE = Path.home() / \"ProyectoFinalVA\"\n",
    "SPLITS_DIR = BASE / \"img_splits\"   # train.txt, val.txt, test.txt (rutas a .npy)\n",
    "COCO_JSONS = [\n",
    "    Path.home() / \"TBBR\" / \"Flug1_100-104Media_coco.json\",\n",
    "]\n",
    "OUT_DIR_JSON = BASE / \"annotations_split\"\n",
    "WRITE_YOLO = True  # poné True si querés generar .txt YOLO\n",
    "YOLO_OUT_ROOTS = {\n",
    "    \"train\": BASE / \"YOLO/HST/labels/train\",  # cambiá si querés GST u otra ruta\n",
    "    \"val\":   BASE / \"YOLO/HST/labels/val\",\n",
    "    \"test\":  BASE / \"YOLO/HST/labels/test\",\n",
    "}\n",
    "# ==========================\n",
    "\n",
    "def load_split_stems():\n",
    "    \"\"\"Lee train/val/test.txt y devuelve sets de 'stem' (nombre base del .npy).\"\"\"\n",
    "    stems = {}\n",
    "    for sp in [\"train\", \"val\", \"test\"]:\n",
    "        txt = SPLITS_DIR / f\"{sp}.txt\"\n",
    "        with open(txt) as f:\n",
    "            files = [l.strip() for l in f if l.strip()]\n",
    "        stems[sp] = {Path(p).stem for p in files}\n",
    "    return stems\n",
    "\n",
    "def load_coco(json_paths):\n",
    "    \"\"\"Fusiona múltiples COCO en un único dict COCO (images, annotations, categories).\"\"\"\n",
    "    merged = {\"images\": [], \"annotations\": [], \"categories\": []}\n",
    "    img_id_offset = 0\n",
    "    ann_id_offset = 0\n",
    "    # Unificar categorías por id (asumimos mismos ids/nombres entre archivos)\n",
    "    cat_seen = {}\n",
    "    for jp in json_paths:\n",
    "        with open(jp) as f:\n",
    "            d = json.load(f)\n",
    "        # categories\n",
    "        for c in d.get(\"categories\", []):\n",
    "            cid = c[\"id\"]\n",
    "            if cid not in cat_seen:\n",
    "                cat_seen[cid] = c\n",
    "    merged[\"categories\"] = list(cat_seen.values())\n",
    "\n",
    "    # images + annotations (conservalo tal cual los ids originales; no reindexamos)\n",
    "    all_images = {}\n",
    "    for jp in json_paths:\n",
    "        with open(jp) as f:\n",
    "            d = json.load(f)\n",
    "        for im in d.get(\"images\", []):\n",
    "            all_images[im[\"id\"]] = im\n",
    "        for an in d.get(\"annotations\", []):\n",
    "            merged[\"annotations\"].append(an)\n",
    "\n",
    "    merged[\"images\"] = list(all_images.values())\n",
    "    return merged\n",
    "\n",
    "def filter_coco_by_stems(coco, keep_stems):\n",
    "    \"\"\"Filtra COCO por el conjunto de stems (coincidencia por file_name.stem).\"\"\"\n",
    "    # Seleccionar imágenes cuyo file_name (sin extensión) esté en keep_stems\n",
    "    keep_img_ids = set()\n",
    "    new_images = []\n",
    "    for im in coco[\"images\"]:\n",
    "        stem = Path(im[\"file_name\"]).stem\n",
    "        if stem in keep_stems:\n",
    "            keep_img_ids.add(im[\"id\"])\n",
    "            new_images.append(im)\n",
    "\n",
    "    # Filtrar annotations por image_id\n",
    "    new_anns = [a for a in coco[\"annotations\"] if a[\"image_id\"] in keep_img_ids]\n",
    "    # Categories quedan igual (las que existen)\n",
    "    return {\"images\": new_images, \"annotations\": new_anns, \"categories\": coco[\"categories\"]}\n",
    "\n",
    "def write_coco(path, coco_obj):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(coco_obj, f)\n",
    "\n",
    "def write_yolo_labels_for_split(coco_split, out_dir):\n",
    "    \"\"\"Escribe .txt YOLO por imagen (formato: cls cx cy w h normalizados).\"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Mapa img_id -> (W,H,stem)\n",
    "    meta = {}\n",
    "    for im in coco_split[\"images\"]:\n",
    "        W, H = im[\"width\"], im[\"height\"]\n",
    "        stem = Path(im[\"file_name\"]).stem\n",
    "        meta[im[\"id\"]] = (W, H, stem)\n",
    "\n",
    "    # Agrupar annotations por image_id\n",
    "    from collections import defaultdict\n",
    "    by_img = defaultdict(list)\n",
    "    for a in coco_split[\"annotations\"]:\n",
    "        by_img[a[\"image_id\"]].append(a)\n",
    "\n",
    "    for iid, anns in by_img.items():\n",
    "        W, H, stem = meta[iid]\n",
    "        lines = []\n",
    "        for a in anns:\n",
    "            cls = a[\"category_id\"]  # asumimos índices de clase ya válidos 0..K-1 (si no, re-mapear)\n",
    "            x, y, w, h = a[\"bbox\"]  # COCO xywh (píxeles)\n",
    "            cx = (x + w/2) / W\n",
    "            cy = (y + h/2) / H\n",
    "            ww = w / W\n",
    "            hh = h / H\n",
    "            cx = min(max(cx, 0.0), 1.0)\n",
    "            cy = min(max(cy, 0.0), 1.0)\n",
    "            ww = min(max(ww, 0.0), 1.0)\n",
    "            hh = min(max(hh, 0.0), 1.0)\n",
    "            lines.append(f\"{cls} {cx:.6f} {cy:.6f} {ww:.6f} {hh:.6f}\")\n",
    "        (out_dir / f\"{stem}.txt\").write_text(\"\\n\".join(lines) + (\"\\n\" if lines else \"\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stems_by_split = load_split_stems()\n",
    "    coco = load_coco(COCO_JSONS)\n",
    "\n",
    "    for sp in [\"train\", \"val\", \"test\"]:\n",
    "        coco_sp = filter_coco_by_stems(coco, stems_by_split[sp])\n",
    "        out_json = OUT_DIR_JSON / f\"{sp}.json\"\n",
    "        write_coco(out_json, coco_sp)\n",
    "        print(f\"[{sp}] images={len(coco_sp['images'])} anns={len(coco_sp['annotations'])} -> {out_json}\")\n",
    "\n",
    "        if WRITE_YOLO:\n",
    "            write_yolo_labels_for_split(coco_sp, YOLO_OUT_ROOTS[sp])\n",
    "            print(f\"[{sp}] YOLO .txt escritos en {YOLO_OUT_ROOTS[sp]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6318d073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python-headless in /home/matiaslein/.local/lib/python3.10/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /home/matiaslein/.local/lib/python3.10/site-packages (from opencv-python-headless) (2.2.6)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python-headless\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2e1ab3",
   "metadata": {},
   "source": [
    "Esta celda convierte el dataset al formato que YOLO necesita para entrenar. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a00305",
   "metadata": {},
   "source": [
    "# Conversion a YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf7b704e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HST -> train:   0%|                                               | 0/487 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HST -> train: 100%|█████████████████████████████████████| 487/487 [13:57<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] generadas: 487, fallidas: 0\n",
      "[train] creados 73 .txt vacíos (para negativos).\n",
      "[train] ✔ imágenes y labels matchean por nombre base.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HST -> val: 100%|███████████████████████████████████████| 139/139 [04:46<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] generadas: 139, fallidas: 0\n",
      "[val] creados 23 .txt vacíos (para negativos).\n",
      "[val] ✔ imágenes y labels matchean por nombre base.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HST -> test: 100%|████████████████████████████████████████| 70/70 [02:27<00:00,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] generadas: 70, fallidas: 0\n",
      "[test] creados 6 .txt vacíos (para negativos).\n",
      "[test] ✔ imágenes y labels matchean por nombre base.\n",
      "✅ Listo. PNGs HST en: /home/matiaslein/ProyectoFinalVA/YOLO/HST/images  |  Labels: /home/matiaslein/ProyectoFinalVA/YOLO/HST/labels\n",
      "YAML: /home/matiaslein/ProyectoFinalVA/YOLO/HST/YOLO.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "BASE = Path.home() / \"ProyectoFinalVA\"\n",
    "SPLITS_DIR = BASE / \"img_splits\"                # train.txt / val.txt / test.txt (rutas a .npy)\n",
    "MODE = \"HST\"                                # \"HST\" o \"GST\"\n",
    "OUT_ROOT = BASE / f\"YOLO/{MODE}\"        # destino de imágenes y YOLO.yaml\n",
    "LABELS_ROOT = OUT_ROOT / \"labels\"           # ya generadas (YOLO .txt)\n",
    "IMAGES_ROOT = OUT_ROOT / \"images\"           # aquí se crearán PNGs\n",
    "THERM_NORM = \"per_image\"                    # \"per_image\" o \"global\"\n",
    "WRITE_YAML = True\n",
    "CLASS_NAMES = None                          # ej: [\"roof\",\"window\",\"wall\"] o None si ya tenés YOLO.yaml\n",
    "# ============================================\n",
    "\n",
    "def ensure(p): Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def robust01(x, lo=1.0, hi=99.0):\n",
    "    lo_v, hi_v = np.percentile(x, [lo, hi])\n",
    "    x = np.clip(x, lo_v, hi_v)\n",
    "    return (x - lo_v) / (hi_v - lo_v + 1e-9)\n",
    "\n",
    "def to_uint8(x):\n",
    "    return (np.clip(x, 0, 1) * 255).astype(np.uint8)\n",
    "\n",
    "def load_list(txt_path: Path):\n",
    "    with open(txt_path) as f:\n",
    "        return [l.strip() for l in f if l.strip()]\n",
    "\n",
    "def compute_global_therm_stats(train_list, band=3, max_files=2000):\n",
    "    vals = []\n",
    "    for p in tqdm(train_list[:max_files], desc=\"stats térmica (global)\", ncols=90):\n",
    "        arr = np.load(p)\n",
    "        t = arr[..., band].astype(np.float32).ravel()\n",
    "        t = t[np.isfinite(t)]\n",
    "        if t.size:\n",
    "            vals.append(t)\n",
    "    if not vals:\n",
    "        return 0.0, 1.0\n",
    "    t_all = np.concatenate(vals)\n",
    "    mu, std = float(np.mean(t_all)), float(np.std(t_all) + 1e-9)\n",
    "    return mu, std\n",
    "\n",
    "def write_yaml(root: Path, names):\n",
    "    yml = f\"\"\"path: {root.resolve()}\n",
    "train: images/train\n",
    "val: images/val\n",
    "test: images/test\n",
    "names: {names if names is not None else []}\n",
    "\"\"\"\n",
    "    (root / \"YOLO.yaml\").write_text(yml)\n",
    "\n",
    "def make_image_from_npy(npy_path: str, out_png: Path, mode: str, mu=None, std=None):\n",
    "    \"\"\"\n",
    "    Crea PNG HST o GST a partir de un .npy con bandas:\n",
    "      0-2: RGB, 3: Térmica, 4: (Depth, no usada)\n",
    "    \"\"\"\n",
    "    arr = np.load(npy_path)\n",
    "    if arr.ndim != 3 or arr.shape[-1] < 4:\n",
    "        return False  # archivo inesperado\n",
    "\n",
    "    rgb = arr[..., :3].astype(np.float32)\n",
    "    t   = arr[..., 3].astype(np.float32)\n",
    "\n",
    "    # Normalización robusta por canal para RGB → uint8\n",
    "    rgb_min = np.percentile(rgb, 1, axis=(0,1))\n",
    "    rgb_max = np.percentile(rgb, 99, axis=(0,1))\n",
    "    rgb_n = (rgb - rgb_min) / (rgb_max - rgb_min + 1e-9)\n",
    "    rgb_u8 = to_uint8(rgb_n)\n",
    "\n",
    "    # HSV / GRAY\n",
    "    hsv = cv2.cvtColor(rgb_u8, cv2.COLOR_RGB2HSV)\n",
    "    h, s, _ = cv2.split(hsv)\n",
    "    gray = cv2.cvtColor(rgb_u8, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Normalización térmica\n",
    "    if THERM_NORM == \"per_image\":\n",
    "        t_n = robust01(t)\n",
    "    else:\n",
    "        t_n = (t - (mu or 0.0)) / ((std or 1.0))\n",
    "        # reescala a 0..1 para guardar\n",
    "        t_n = (t_n - t_n.min()) / (t_n.max() - t_n.min() + 1e-9)\n",
    "    t_u8 = to_uint8(t_n)\n",
    "\n",
    "    # Composición\n",
    "    if mode.upper() == \"HST\":\n",
    "        out = cv2.merge([h, s, t_u8])     # H,S,T\n",
    "    else:\n",
    "        out = cv2.merge([gray, s, t_u8])  # Gray,S,T\n",
    "\n",
    "    ensure(out_png.parent)\n",
    "    cv2.imwrite(str(out_png), out)\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    # YAML (si no existe y pedís generarlo)\n",
    "    if WRITE_YAML and not (OUT_ROOT / \"YOLO.yaml\").exists():\n",
    "        write_yaml(OUT_ROOT, CLASS_NAMES)\n",
    "\n",
    "    # Cargar listas\n",
    "    split_files = {}\n",
    "    for sp in [\"train\", \"val\", \"test\"]:\n",
    "        p = SPLITS_DIR / f\"{sp}.txt\"\n",
    "        if not p.exists():\n",
    "            print(f\"⚠️ Falta {p} — salto split {sp}\")\n",
    "            continue\n",
    "        split_files[sp] = load_list(p)\n",
    "\n",
    "    # Stats globales (opcional)\n",
    "    mu = std = None\n",
    "    if THERM_NORM == \"global\" and \"train\" in split_files:\n",
    "        mu, std = compute_global_therm_stats(split_files[\"train\"])\n",
    "\n",
    "    # Procesar cada split\n",
    "    for sp, npy_list in split_files.items():\n",
    "        out_dir = IMAGES_ROOT / sp\n",
    "        ensure(out_dir)\n",
    "        # Si existen labels, los usamos para chequear nombres más adelante\n",
    "        labels_dir = LABELS_ROOT / sp\n",
    "\n",
    "        ok, bad = 0, 0\n",
    "        for p in tqdm(npy_list, desc=f\"{MODE} -> {sp}\", ncols=90):\n",
    "            stem = Path(p).stem\n",
    "            out_name = f\"{stem}.png\"   # coincide con .txt: <stem>_<MODE>.txt\n",
    "            out_png = out_dir / out_name\n",
    "            if out_png.exists():  # evitar recomputar\n",
    "                ok += 1\n",
    "                continue\n",
    "            if make_image_from_npy(p, out_png, MODE, mu, std):\n",
    "                ok += 1\n",
    "            else:\n",
    "                bad += 1\n",
    "\n",
    "        print(f\"[{sp}] generadas: {ok}, fallidas: {bad}\")\n",
    "\n",
    "                # Verificación rápida con labels (si existen)\n",
    "        # Verificación rápida con labels (si existen)\n",
    "        if labels_dir.exists():\n",
    "            # normalización robusta de stems\n",
    "            def norm(s: str) -> str:\n",
    "                s = s.strip()\n",
    "                for suf in (\"_HST\", \"_GST\", \"_hst\", \"_gst\"):\n",
    "                    if s.endswith(suf):\n",
    "                        return s[: -len(suf)]\n",
    "                return s\n",
    "\n",
    "            imgs = {norm(q.stem) for q in out_dir.glob(\"*.png\")}\n",
    "            labs = {norm(q.stem) for q in labels_dir.glob(\"*.txt\")}\n",
    "\n",
    "            # 1) Crear primero los .txt vacíos para los que falten\n",
    "            faltan = sorted(list(imgs - labs))\n",
    "            for stem in faltan:\n",
    "                (labels_dir / f\"{stem}_{MODE}.txt\").touch()\n",
    "            if faltan:\n",
    "                print(f\"[{sp}] creados {len(faltan)} .txt vacíos (para negativos).\")\n",
    "\n",
    "            # 2) Recalcular sets y reportar estado final (ya sin falsos warnings)\n",
    "            labs = {norm(q.stem) for q in labels_dir.glob(\"*.txt\")}\n",
    "            faltan_txt = sorted(list(imgs - labs))[:10]\n",
    "            faltan_img = sorted(list(labs - imgs))[:10]\n",
    "\n",
    "            if faltan_txt:\n",
    "                print(f\"⚠️ En {sp}: hay imágenes sin .txt (primeros 10): {faltan_txt}\")\n",
    "            if faltan_img:\n",
    "                print(f\"⚠️ En {sp}: hay .txt sin imagen (primeros 10): {faltan_img}\")\n",
    "            if not faltan_txt and not faltan_img:\n",
    "                print(f\"[{sp}] ✔ imágenes y labels matchean por nombre base.\")\n",
    "\n",
    "\n",
    "    print(f\"✅ Listo. PNGs {MODE} en: {IMAGES_ROOT}  |  Labels: {LABELS_ROOT}\")\n",
    "    if WRITE_YAML:\n",
    "        print(f\"YAML: {OUT_ROOT/'YOLO.yaml'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
